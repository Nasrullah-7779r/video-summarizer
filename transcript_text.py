test_transcript = "Let's go ahead and talk about the AI terminal war that's going on. Literally, every single LLM company, no matter what the size and scale they are on, they want to grab your keyboard's attention on terminal. They want the full access of your terminal somehow and want to control all of it. Now, this war was usually going on in the editor. There was a long debate and long war onto that. Now all of them are suddenly jumping into the terminal. I don't know what's the reason for that but somehow they are all now in the terminal. So in this video I will give you my opinion. What is my feeling and what did I observed while using all of them. Yes, I literally used all of them in the last week or so to figure out which one is suiting the best. And for all of them I didn't use the traditional way of using React, Nex.js or Node. I actually tested them with not so popular applications to build out something like elixir something like Ruby on Rails. Yes, I know Ruby on Rails is popular but I wanted to test these application that they can they actually give me a finished product and I tried and tested with all of them. Now the product which I tested doesn't really matter. What matter is what was the experience behind using them and of course certain of these products are really good in some cases certain of them are not really good in certain cases. So not only that I have actually gone ahead and have prepared a chart where I will be discussing you what are the pros and cons on all of them and before that we'll also talk about the cursor. Now one thing which is absolutely sure about these terminal war is things are moving and things are changing in every single LLM company. What's changing? All of them probably not all of them but almost all of them have realized that the most amount of money that they are making is through developers. The LLM was supposed to solve a lot of use cases probably in the medical probably in in the legal section. But right now where it is thriving and is actually making money is in the developer ecosystem. If you look at the graph and chart which are easily available online, Enthropic is one such company which is making the most amount of money and we all know why Enthropic is famous for generating the code output. That's where it is. So I have my opinions and also how these companies can actually shake the future of products like cursor windsurf and many others as well. Now there are a lot of player in this tray cursor windsurf and so many more as well. I happen to just find out that what's going on with the cursor and how these things are actually changing and probably changing everything in the future of terminal as well as editor. I have a lot to say but I'll take you on to the screen so that we can actually discuss them one by one of how these things actually occur and what's going on. All right. So starting with absolute basic that how this video came up. I recently saw this announcement on the cursor. I discussed this in one of my live stream as well on different channel that hey this is the cursor update. Look at this. The update says we recently updated our pricing but missed the mark. We are refunding the affected customer and clarifying up how pricing works. That's great. Now, if you go and look at the cursor and you click on the pricing page, this is how they have changed the pricing. Now, right now, it says free here and in the $20, it says previously before making this video, it used to say unlimited, unlimited, unlimited, unlimited. Everything was unlimited. But this thing of uh fair usage limit was actually probably in kind of a eye button or was somewhere hidden in the pricing policies or something like that. But now they have made it very clear that the $20 per month is not cutting it through. This is just a hobby plan. If you really are a pro developer, you should be switching to $200 a month. And you might be thinking that cursor wants to make a lot of money with this. Probably true. But this is not possible for the cursor because the LLM behind the scene which are powering the cursor, they have also raised their prices and cursor can't do much in this. Notice now how they say that it's extended limits on agent, unlimited tab completion and access to background agent. How much? That's where the interesting thing comes up. Now they're clarifying that how this pricing and everything works. So they mention it how many requests. So if you'll see this there are there was a 500 request limit per month with sonnet model costing two request. So what they're saying that it's let me just summarize this. It's mostly 500 request and they mention it here as well. Uh we were not clear that unlimited usage was only for auto and not for all other models which have at least $20 for usage plan. Based on the media token, the pro plan currently covers 225 sonnet 4 request, 550 Gemini request and 650 GPT4. Now this is a lot. There's a lot that you might not be able to grasp, but what I'm saying is now if you want to fully thrively use cursor, you have to probably switch to $200 per month. Why so? because Enthropic changed their pricing. Whatever was previously discussed as that we are reserving this much of the limits or tokens or request uh that's got expensive. Now if you look at the Twitter a lot of people are not happy. Reddit is not happy that how they could have actually done that. A lot of people lost money then they are refunding it. There are a lot of things like just canceled my cursor subscription after a year of usage. It's clear that if you don't fully own your software and AI stack, you will be subject to companies disturbing your workflow. This guy actually put it really nicely. He totally understand that this is not totally a fault of cursor but rather a fault uh of the AI stack that they probably are not owning. Okay, this is the basics of the premises that all right this is all going on. So if you go ahead and look onto this premises now. So we have seen that companies like cursor and let me expand this companies like cursor there we go companies like cursor are just not fully in control and there are others as well like cursor is one of them we have wind surf as well now what they do is they definitely write their own prompts their own engineering but basically they actually wrap everything up uh for the LLMs yep That's the LLM. Now, whose LLM? That's the major question. The LLM here could be of Google, could be of Anthropic, could be of OpenAI, any one of them. And eventually, these companies realize that instead of offering our services from LLM to to any of these providers, can we go ahead and provide the service directly? And if they provide the service directly, they probably have to go another fork of the VS code. But they realized why to go for this route. Why to have a VS code fork when we can actually provide all of this thing via terminal app? Terminal app is super simple to build compared to the VS code or any editor. You just get the whole access to the folder and that's it. Whatever the windsurer or cursor were doing, I can just do that in the terminal and hence these LLM companies wants to disturb the entire flow of cursor or windsf or any other rapper that they're offering because they also know that the most amount of money that they're making is actually in the coding. So this brings us to the most important question that how many of these players are and where actually things move on. So this is my warp and what you will notice I was using Gemini onto this. So notice here this is a Gemini prompt and I was working on some stuff trying to figure out the stuff. Now with this this is the Gemini pro model and Google being Google they are offering kind of generous amount of tier for free uh for a lot of use cases but this is the part where I was not so happy. I'll come on to that part as well. Not only that, I have actually used insane amount of tokens on the enthropic as well to fully understand that what is the capability and where these models shines and where these models lag because without using them talking about would be just an unnecessary opinion. Now that I have extendedly used them Gemini claude as well as warp now I have some strong opinions to work on with that. First of all let's come on to this. First and foremost, what I will suggest you, there is a new contender here which is warp. And warp automatically provides you the terminal access to all of these LM models directly. If you go ahead and look at into the warp here, what you will notice at the very bottom, you have these mode. You can directly go ahead and switch into auto mode, the agent mode, terminal mode. They have a lot of mode. I'll show you that as well. But what you will notice, you have direct access to light models, claud. through this one instance you get access to everything and if you happen to love terminal they are just right there from Gemini to GBT to claude every single model is up here. Now there are certain advantages and certain disadvantages with this as well but yes I would say and by the way if you click on the very top you have agents as well installing agents code agents deploy agent and to be honest where the warp thrives is the deploy agent. This is hands down the best agent that I have seen in all of them. Now for the code, I prefer to use claude because the output was of a much much higher quality than anybody else. But where it shines the warp is the deploy. So let's go ahead and talk about them that where all of these actually shines and where it works. So for the warp, I would say that if you don't want to buy insane high amount of tokens by claude, then warp is your choice. It's definitely better than Gemini. So for the code, I prefer warp. And apart from the code, what I would prefer is deploy. Hands down, nothing comes nearby when it comes to the deploy agent. So warp actually makes sense into this segment. Then the next one is Claude. Claude really shines in the in the code. You ask it to any code uh segment to deploy any project. It generates it nicely, gets the to-dos nicely and follow these to-dos nicely. Now in some cases you might have to tweak up it and what I noticed if I tweak up these small things it saves token but if I just rely totally on that hey you fix everything oh man it goes really really expensive in that case. So make sure you keep control of the project whatever you are doing but you can rely on the code part more on the claude. Where I was bit of a disappointed was in the Gemini. I'll show you more of the graph chart as well. But this is where I uh was heavily disappointed. First of all, Google has made this kind of a narration that Gemini is the most cheapest of all. It is not. If you use the pro models of it, it is kind of decently expensive. Not as far as the claude, but almost very near to it. You will find that once you lose the free tier, then after that it's super super expensive. So for this, I was okayish. If your tech stack is more of like a JS stack or TS stack, it works kind of a nicely. But moving anything outside of it, the Gemini actually struggled in giving me the project entirely. Claude thrived it. Warped also worked seamlessly good. Deployment, Git, anything that you do on the server for infrastructure as a code for anything like that. Oh man, Warp just thrives it nicely. And what I really liked about is that I don't have to pay for any separate tokens. I can just I'm anyways using for by the way if you don't want to see if you want to see that as well this is how I run the gemini I hit enter and gemini actually says that hey this is I am you want to give this anyways I'm using uh claw I'm using warp for everything so I can hit ctrl c and there we go this is how it works and you can switch into the agent mode you can switch into the audio I haven't tried the audio one but there is also a git repository there's attachment mode and all of them what I like about it is I can easily switch into different models and by the time you will be using all the pro models just from here, you probably will be done with the project for at least for that month or that day. On top of that, I really like that if you go into their settings, they actually give this billing and usage openly up here. So, you can compare the plan, you can upgrade the plan and their pricing plan. I like that because it is very open in saying what I do. I don't do anything unlimited. I do in the fair region so that they don't attract any controversies. They simply say that the pro model is 2,500 warp AI request, 40 indexed code bases, uh 10,000 files code bases. So it's very clear that what they will be doing, what they will be charging and the turbo plan is also very clear. So I like that part. Now coming back on to the part where I have drawn this nice uh chart for you to make sure you understand what's going on with this. So I have few contenders here. The first one being warp, the second one being Gemini Claude and the co-pilot. Yes, I tried copilot as well. It was decent. I was not that much happy. But there are some things which are uh great in this one. First of all, AI command generation. And by the term AI command generation. I have tested it for generating the code as well. Uh that's a little bit different. But AI command generation means I want to generate a command for ffmpeg or I want to generate a command to connect with my server or to install docker in my remote machine. Command generation warp was hands down the best model. Gemini was also able to do it decently but warp did it much faster and much much accurately. On the other hand, Claude also did it absolutely in the very first go nicely. Copilot worked there but after giving it a small directions and hint it worked out nicely. So anything that goes onto other remote server Bob hands down beats everything else. Then comes the error explanation. Now here's the interesting part. If your error is in the code itself while running the code and if it is in the uh browser or something then Gemini actually fails really bad here. It doesn't understand it. I tried it left and right but it doesn't well go well. Claude was able to understand it decently. But where the warp shines is if the if the error comes up rightly in the terminal itself, warp will auto suggest it and try to fix it because they have the context of the terminal already. So this is where if the error is directly in the terminal and any hint is given in the terminal, that's it. WP is going to go ahead and kill it up. Now GitHub copilot worked okay. Not very happy with that. Now another thing is cloud integration. And you might be wondering what's cloud is it the accessibility of the cloud? No, it's a cloud integration. Now, we know that some people work with AWS, some people work with GCP, some people work with a git. So, for that kind of a thing, some of these companies happen to own the cloud infrastructure as well. In that case, the cloud integration was missing in the war because they don't own any cloud company. No hosting company is owned by them. At least as of I know that. So, there was no present of it. But in the Gemini, they actually kind of support GCP as well. And it was decent. It was surprisingly decent in that case. Now Claude also doesn't own any cloud infrastructure company. So they have zero support. And GitHub, I wouldn't call it as a cloud infrastructure, but GitHub copilot CLI is actually really nice with the GitHub integration. All the GitHub uh CI/CD actions as well as all the regular push command, pull commands on all of them, they actually perform really well. Next up is security sandbox. Now security sandbox is really interesting. Now some of your commands or some of your projects needs to run in the sandbox environment. Uh I realize that the security sandbox in the excellent loved it in the Gemini again super good. But where the Gemini actually gets confused, if you choose the option of giving them the API keys, they expect that the KPI API keys will be available in the system or you will be having av file which will have a Google Gemini API key equals to the keys itself. Now when you will be generating a project in the same directory then it will get confused. Hey this folder is not empty. I should move this here. I should move that there. So this is where I found it a little bit struggling. Now claude I didn't found any resource so that's why the exclamatory sign that whether GitHub copilot and claude actually works well on the security sandbox or not because once I give them the option they were thriving into that uh but again once I terminated the session I started again the session they were asking me the permission again I'm not really sure that what's going on in that case so no comments on that part now offline support now once you turn off the internet some people still want to write the code or maybe in the flight mode maybe in aeroplane wherever they are working on verb has zero support on that once you cut off the internet that's it they are gone and so is for Gemini and so is for claude and so is for co-pilot I think there is a chance of some open-source ondevice model here where these kinds of things might still be working and you can code it and probably you can save some tokens as well if the LLM is running on your system but again I haven't tried I will definitely try in the future about the lama and stuff that hey these kinds of terminal give them the access and I would of that that eventually in the future if I get an option here that hey use your own offline model itself although these are really a lot of them but if I get an option of use your own offline model and give a URL oh man within the terminal all superpower I think that is one place where I can love it now learning from you usage now warp has the cloud infrastructure and what you can do in the warp itself in the settings you will see that there are options of saving some of your blocks so you can save code and all these things. So there are AI settings of this how much permissions you want to give and how much uh save it saving of the things you can do. Now if you give them permission that uh we want to learn from your usage uh warp can do that but you can stop them as well. The same goes for Gemini as well. Uh but again for the claude and copilot I didn't found any permission or anything like that. Uh if you want to have the learning from your usage again this is a part where I would say it's a debatable thing. uh warp you can totally just shut it off and they will not do anything hopefully. Uh but for the Gemini Claude and Copilot uh Gemini says that we don't learn anything from your usage. I I just took their words for this and I said okay it if you're saying it it's all good no information on the claude no information on the co-pilot hence the uh lang hence the icons here. So coming back on to the point that yes I have given you this exact model that where the things are working for me at least the where the warp is shining where the cloud is shining and where the Gemini is shining and again really do understand that if you use this much of token it actually cost you money. So if I go ahead and check my cost yes it does cost decent amount of money. Ah yes it's still costing me but I was able to consume this much amount of money because it was working if it wouldn't be working like in the case of Gemini I was not able to consume that much of token because I had to strive into the direction take my steering give it a direction so much of the work but in the case of warp yes I consumed a lot of token because I was able to and in the claude as well I was able to consume a whole lot of token because I was able to so my general understanding is now the whole ecos system and the whole war is shifting. Previously they were all fighting into the code editor itself because remember they are only making the money in the code ecosystem right now nothing else. So this war definitely will stay in your code editor but now they want everything where the developers are relying or are spending some amount of time apart from code editor developers spend a lot of time in the terminal. So that's obvious place that everybody wants to take some pie out of it. But again, uh this is just opinions and I love to share these kinds of thoughts and experiment with you. Hope you have enjoyed this. That's it for this video. Let's go ahead and catch up in the next one"